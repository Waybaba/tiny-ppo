description: AMLT

target:
  service: singularity
  name: msrresrchvc    # more GPUs 

environment:
  image: waybaba/rl:v2
  username: waybaba
  setup:
    - echo "setup start..."
    - export UPRJDIR=/mnt/default/
    - export UDATADIR=/mnt/storage/data
    - export UOUTDIR=/mnt/storage/output
    - mkdir -p /mnt/storage/output /mnt/storage/data
    - echo "setup finished!"


code:
  local_dir: $CONFIG_DIR/../../

storage:
  input:
    storage_account_name: resrchvc4data
    container_name: v-wangwei1
    mount_dir: /mnt/storage
    local_dir: /home/v-wangwei1/storage

# search:
#   job_template:
#     name: delay_{experiment_name:s}_{auto:5s}
#     command:
#     - python agent/sac.py
#       train_collector.buffer.size={train_collector_buffer_size}
#       policy._target_=tianshou.policy.SACPolicy
#       env.delay={env_delay}
#       seed={seed}
#       tags=["delay_amlt_test_2"]
#   type: grid
#   max_trials: 10000
#   params:
#     - name: train_collector_buffer_size
#       values: [1000000,100000,10000]
#     - name: env_delay
#       values: [1,4,8,16,2,32,0]
#     - name: seed
#       values: [0,1,2,3,4]


# search:
#   job_template:
#     name: delay_amlt_rnn_and_normal{experiment_name:s}_{auto:5s}
#     command:
#     - python agent/sac.py
#       train_collector.buffer.size={train_collector_buffer_size}
#       policy._target_=agent.sac.AsyncACSACPolicy
#       env.delay={env_delay}
#       net={net}
#       net@net_c1={net}
#       seed={seed}
#       tags=["delay_amlt_rnn_and_normal"]
#   type: grid
#   max_trials: 10000
#   params:
#     - name: train_collector_buffer_size
#       values: [1000000,100000,10000]
#     - name: env_delay
#       values: [1,4,8,16,2,32,0]
#     - name: seed
#       values: [0,1,2,3,4]
#     - name: net
#       values: [default,rnn]

# # mine_with_params_as_donqi
# search:
#   job_template:
#     name: delay_amlt_rnn_and_normal{experiment_name:s}_{auto:5s}
#     command:
#     - python agent/sac.py
#       train_collector.buffer.size={train_collector_buffer_size}
#       policy._target_=agent.sac.AsyncACSACPolicy
#       env.delay={env_delay}
#       net={net}
#       net@net_c1={net}
#       seed={seed}
#       tags=["mine_with_params_as_donqi"]
#   type: grid
#   max_trials: 10000
#   params:
#     - name: train_collector_buffer_size
#       values: [1000000]
#     - name: env_delay
#       values: [1,4,8,16,2,32,0]
#     - name: seed
#       values: [0,1,2,3,4]
#     - name: net
#       values: [default]


# # mine_with_params_as_donqi_plus_net
# search:
#   job_template:
#     name: delay_amlt_rnn_and_normal{experiment_name:s}_{auto:5s}
#     command:
#     - python agent/sac.py
#       train_collector.buffer.size={train_collector_buffer_size}
#       policy._target_=agent.sac.AsyncACSACPolicy
#       env.delay={env_delay}
#       net={net}
#       net@net_c1={net}
#       seed={seed}
#       tags=["mine_with_params_as_donqi_plus_net"]
#   type: grid
#   max_trials: 10000
#   params:
#     - name: train_collector_buffer_size
#       values: [1000000]
#     - name: env_delay
#       values: [1,4,8,16,2,32,0]
#     - name: seed
#       values: [0,1,2,3,4]
#     - name: net
#       values: [default]


# search:
#   job_template:
#     name: delay_amlt_rnn_and_normal{experiment_name:s}_{auto:5s}
#     command:
#     - python agent/sac.py
#       train_collector.buffer.size={train_collector_buffer_size}
#       policy._target_=agent.sac.AsyncACSACPolicy
#       env.delay={env_delay}
#       net={net}
#       net@net_c1={net}
#       seed={seed}
#       tags=["mine_with_params_as_donqi_plus_net_trainable_alpha_lr_change"]
#   type: grid
#   max_trials: 10000
#   params:
#     - name: train_collector_buffer_size
#       values: [1000000]
#     - name: env_delay
#       values: [1,4,8,16,2,32,0]
#     - name: seed
#       values: [0,1,2,3,4]
#     - name: net
#       values: [default]

# # mine_with_params_as_donqi_plus_net_trainable_alpha_lr_change_reg_7
# search:
#   job_template:
#     name: delay_amlt_rnn_and_normal{experiment_name:s}_{auto:5s}
#     command:
#     - python agent/sac.py
#       policy.critic_use_oracle_obs={critic_use_oracle_obs}
#       collector.train_collector.buffer.size={train_collector_buffer_size}
#       policy._target_=agent.sac.AsyncACSACPolicy
#       env.delay={env_delay}
#       env.name={env_name}
#       net={net}
#       net@net_c1={net}
#       seed={seed}
#       tags=["START_complex"]
#   type: grid
#   max_trials: 10000
#   params:
#     - name: train_collector_buffer_size
#       values: [1000000]
#     - name: env_delay
#       values: [1,4,8,16,2,32,0]
#     - name: seed
#       values: [0,1,2,3,4]
#     - name: critic_use_oracle_obs
#       values: [true]
#     - name: net
#       values: [default]
#     - name: env_name
#       # values: [HalfCheetah-v4]
#       values: [Ant-v4,Hopper-v4,Walker2d-v4]

search:
  job_template:
    name: RL_Delayed_{experiment_name:s}_{auto:5s}
    command:
    - python src/entry.py
      -m
      hydra/launcher=wsl_parallel
      n_jobs={n_jobs}
      seed={seed}
      experiment={experiment}
      env.name={env_name}
      trainer.batch_size={trainer_batch_size}
      actor.net.mlp_hidden_sizes={actor_mlp_hidden_sizes}
      actor.net.rnn_layer_num={actor_rnn_layer_num}
      actor.net.rnn_hidden_layer_size={actor_rnn_hidden_layer_size}
      critic1.net.mlp_hidden_sizes={critic_mlp_hidden_sizes}
      critic1.net.rnn_layer_num={critic_rnn_layer_num}
      critic1.net.rnn_hidden_layer_size={critic_rnn_hidden_layer_size}
      global_cfg.actor_input.history_merge_method={actor_history_merge_method}
      global_cfg.actor_input.obs_type={actor_obs_type}
      global_cfg.actor_input.history_num={actor_history_num}
      global_cfg.actor_input.burnin_num={actor_burnin_num}
      global_cfg.actor_input.noise_act_debug={noise_act_debug}
      global_cfg.critic_input.history_merge_method={critic_history_merge_method}
      global_cfg.critic_input.obs_type={critic_obs_type}
      global_cfg.actor_input.obs_pred.middle_detach={middle_detach}
      global_cfg.actor_input.obs_pred.net.net.mlp_hidden_sizes={obs_pred_mlp_hidden_sizes}
      collector.train_collector.buffer.size={buffer_size}
      env.delay={env_delay}
      tags=[{tag}]
  type: grid
  max_trials: 10000
  params:
    - name: env_delay
      # values: [0]
      values: [0,1,2,4]
      # values: ["0,2","4,8","16,32"]
      # values: ["0,2,4,8,16,32"]
      # values: [0,2,4]
    - name: env_name
      # values: [HalfCheetah-v4]
      # values: [HalfCheetah-v4]
      values: [HalfCheetah-v4,Ant-v4,Hopper-v4,Walker2d-v4]
    - name: experiment
      values: [sac_cat]
    - name: actor_mlp_hidden_sizes
      values: ["[256,256]"]
    - name: actor_rnn_layer_num
      values: [0]
    - name: actor_rnn_hidden_layer_size
      values: [256]
    - name: critic_mlp_hidden_sizes
      values: ["[256,256]"]
    - name: critic_rnn_layer_num
      values: [0]
    - name: critic_rnn_hidden_layer_size
      values: [256]
    - name: obs_pred_mlp_hidden_sizes
      values: ["[256,256,256]"]
    - name: middle_detach
      values: [true]
    - name: actor_history_merge_method
      values: ["cat_mlp"]
    - name: actor_obs_type
      values: ["normal"]
    - name: actor_history_num
      values: [1,2,4,8]
    - name: actor_burnin_num 
      values: [0.25]
    - name: noise_act_debug
      values: [false]
    - name: critic_history_merge_method
      values: ["none"]
    - name: critic_obs_type
      values: ["oracle"]
      ### cluster related
    - name: trainer_batch_size
      values: [256]
    - name: buffer_size
      values: [1000000]
    - name: seed
      values: ["0,1,2,3,4"]
    - name: n_jobs
      values: [5]
    - name: tag
      values: ["pred_obs_predOuputBugFix"] 
    