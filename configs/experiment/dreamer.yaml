# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - default.yaml
  - /net@dreamer.net_mlp: mlp_basic.yaml
  - /net@dreamer.net_rnn: rnn_basic.yaml

# common - for all tasks (task_name, tags, output_dir, device)
policy:
  update_a_per_c: 2
  clip: 1.0
  # noise_decay_rate: 0.998
  noise_decay_rate: 1.0
  policy_noise: 0.2 # training: noise = torch.randn() -> noise *= self.policy_noise -> noise = noise.clamp(-self.noise_clip, self.noise_clip)
  noise_clip: 0.5
  # initial_exploration_noise: 0.1
  # exploration_noise: # infer env: 
  #   _target_: tianshou.exploration.random.GaussianNoise
  #   sigma: 0.1
  initial_exploration_noise: 0.1 # infer env: # use initial_exploration_noise since self.exploration_noise would be updated


actor:
  unbounded: true
  conditioned_sigma: true
actor_optim:
  lr: 1e-3 # 1e-3 in tianshou, 3e-4 in dongqi code. (For both actor and critic)

dreamer:
  _target_: src.runner.CustomDreamer
  _partial_: true
  device: ${device}
  net_mlp: ???
  net_rnn: ???

dreamer_optim:
  _target_: torch.optim.Adam
  _partial_: true
  lr: ${actor_optim.lr}
global_cfg:
  dreamer_input:
    history_merge_method: "none" # cat_mlp or stack_rnn or none
    obs_type: "normal" # normal or oracle

runner:
  _target_: src.runner.DreamerRunner


start_timesteps: 10000

algorithm_name: sac