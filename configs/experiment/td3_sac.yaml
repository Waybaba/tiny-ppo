# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - /net@net_c1: default.yaml
  - /actor: null
  - _self_

# common - for all tasks (task_name, tags, output_dir, device)
# SAC specific

global_cfg: # would be passed to actor, critic1, critic2, policy, env
  actor_input:
    history_merge_method: "cat_mlp" # cat_mlp or stack_rnn or none
    obs_type: "normal" # normal or oracle
    trace_direction: "prev" # prev or next
    custom_return_cal: false 
    history_num: ${env.delay} # decide the info["historical_act"] length # would use ${env.delay}
    noise_act_debug: false
    obs_pred:
      turn_on: false 
      feat_dim: 256
      middle_detach: false 
      input_type: "feat" # obs or feat
      net_type: mlp # vae or mlp
      norm_kl_loss_weight: 10.0
      auto_kl_target: 50
      pred_loss_weight: 0.1
      auto_kl_optim:
        _target_: torch.optim.Adam
        _partial_: true
        lr: ${actor_optim.lr}
      optim:
        _target_: torch.optim.Adam
        _partial_: true
        lr: ${actor_optim.lr}
      net:
        _target_: src.runner.ObsPredNet
        _partial_: true
        device: ${device}
        feat_dim: ${global_cfg.actor_input.obs_pred.feat_dim}
        net_type: ${global_cfg.actor_input.obs_pred.net_type}
        encoder_net: 
          _target_: src.runner.RNN_MLP_Net
          _partial_: true
          rnn_layer_num: 0
          rnn_hidden_layer_size: 256 
          mlp_hidden_sizes: [256,256]
          mlp_softmax: false
        decoder_net:
          _target_: src.runner.RNN_MLP_Net
          _partial_: true
          rnn_layer_num: 0
          rnn_hidden_layer_size: 256
          mlp_hidden_sizes: [256]
          mlp_softmax: false
    obs_encode:
      turn_on: true 
      feat_dim: 256
      train_eval_async: true # true is VLOG, false is normal training. only affect the training input
      before_policy_detach: false # whether detach the input before policy net. Detail: there is no certain answer for which one is better.
      norm_kl_loss_weight: 0.1
      auto_kl_target: 50
      pred_loss_weight: 0.0
      optim:
        _target_: torch.optim.Adam
        _partial_: true
        lr: ${actor_optim.lr}
      auto_kl_optim:
        _target_: torch.optim.Adam
        _partial_: true
        lr: ${actor_optim.lr}
      net:
        _target_: src.runner.ObsEncodeNet
        _partial_: true
        feat_dim: ${global_cfg.actor_input.obs_encode.feat_dim}
        device: ${device}
        encoder_net: 
          _target_: src.runner.RNN_MLP_Net
          _partial_: true
          rnn_layer_num: 0
          rnn_hidden_layer_size: 256
          mlp_hidden_sizes: [256,256]
          mlp_softmax: false
        decoder_net:
          _target_: src.runner.RNN_MLP_Net
          _partial_: true
          rnn_layer_num: 0
          rnn_hidden_layer_size: 256
          mlp_hidden_sizes: [256,256]
          mlp_softmax: false
  critic_input:
    history_merge_method: "none" # cat_mlp or stack_rnn or none
    obs_type: "oracle" # normal or oracle
  log_interval: 100
  log_instant_commit: true # false would make the wandb sync only once per epoch



trainer:
  batch_size: 256 # 256,1 for normal, 32,64 for donqi
  batch_seq_len: 1 # would be used in buffer.seq_len 
  # batch_size: 32
  # batch_seq_len: 64
  test_in_train: false
  episode_per_test: 10
  max_epoch: 200
  step_per_epoch: 5000 # epoch is eval
  # max_epoch: 10
  # step_per_epoch: 1000 # epoch is eval
  step_per_collect: 1
  update_per_step: 1
  log_interval: 100
  log_upload_interval: 100 # 0 for instant upload
  progress_bar: true
  hide_eval_info_print: ${trainer.progress_bar}

policy:
  tau: 0.005
  gamma: 0.99

runner:
  _target_: ???
  _partial_: true

actor:
  _target_: src.runner.CustomRecurrentActorProb
  _partial_: true
  device: ${device}
  net: 
    _target_: src.runner.RNN_MLP_Net
    _partial_: true
    rnn_layer_num: 0
    rnn_hidden_layer_size: 256
    mlp_hidden_sizes: [256,256]
    mlp_softmax: false
  unbounded: ???
  conditioned_sigma: ???
  pure_random: false
critic1:
  _target_: src.runner.CustomRecurrentCritic
  _partial_: true
  device: ${device}
  net: 
    _target_: src.runner.RNN_MLP_Net
    _partial_: true
    rnn_layer_num: 0
    rnn_hidden_layer_size: 256
    mlp_hidden_sizes: [256,256]
    mlp_softmax: false
critic2: ${critic1}
actor_optim:
  _target_: torch.optim.Adam
  _partial_: true
  lr: ???
critic1_optim:
  _target_: ${actor_optim._target_}
  _partial_: true
  lr: ${actor_optim.lr}
critic2_optim: ${critic1_optim}


# common - for all algorithms
buffer: 
  _target_: src.runner.ReplayBuffer
  size: 1200000
  seq_len: ${trainer.batch_seq_len} # would affect the remaster buffer gap len
env_max_step: 5000
env:
  name: HalfCheetah-v4
  train_num: 1
  test_num: 1
  delay: 0
collector:
  train_collector:
    _target_: tianshou.data.Collector
    _partial_: true
    buffer: 
      _target_: tianshou.data.ReplayBuffer
      size: 1000000
      # stack_num: 8
      # buffer_num: 2
    exploration_noise: true
  test_collector:
    _target_: tianshou.data.Collector
    _partial_: true
start_timesteps: ???

# common - for all tasks (task_name, tags, output_dir, device)
task_name: "RL_Apr2"
tags: ["debug"]
