# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - td3.yaml
  - policy: null

# common - for all tasks (task_name, tags, output_dir, device)
policy:
  alpha:
    _target_: builtins.tuple
    _args_:
      - - "neg_act_num" # target_entropy - float -6.0 or str "neg_act_num"
        - _target_: torch.zeros
          size: 
            _target_: builtins.tuple
            _args_:
              - - 1
          requires_grad: true
          device: ${device}
        - _target_: torch.optim.Adam
          _partial_: true
          lr: 3e-4 # 3e-4 in tianshou
actor_optim:
  lr: 1e-3 # 1e-3 in tianshou, 3e-4 in dongqi code. (For both actor and critic)

runner:
  _target_: src.runner.SACRunner
