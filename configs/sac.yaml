defaults:
  - hydra: default.yaml
  - paths: default.yaml
  - env: HalfCheetah.yaml
  - _self_
# info
task_name: "RL"
tags: ["debug"]

output_dir: "${paths.output_dir}"
device: "cuda"
net:
  _target_: tianshou.utils.net.common.Net
  _partial_: true
  hidden_sizes: [64, 64]
  softmax: false
  device: ${device}
actor:
  _target_: tianshou.utils.net.continuous.ActorProb
  _partial_: true
  device: ${device}
actor_optim:
  _target_: torch.optim.Adam
  _partial_: true
  lr: 3e-4
net_c1:
  _target_: tianshou.utils.net.common.Net
  _partial_: true
  hidden_sizes: [64, 64]
  concat: true
  device: ${device}
critic1:
  _target_: tianshou.utils.net.continuous.Critic
  _partial_: true
  device: ${device}
critic1_optim:
  _target_: torch.optim.Adam
  _partial_: true
  lr: 3e-3
net_c2:
  _target_: tianshou.utils.net.common.Net
  _partial_: true
  hidden_sizes: [64, 64]
  concat: true
  device: ${device}
critic2:
  _target_: tianshou.utils.net.continuous.Critic
  _partial_: true
  device: ${device}
critic2_optim:
  _target_: torch.optim.Adam
  _partial_: true
  lr: 3e-3
policy:
  # _target_: tianshou.policy.SACPolicy
  # _target_: agent.sac.CustomSACPolicy
  _target_: agent.sac.AsyncACSACPolicy
  _partial_: true
  tau: 0.005
  gamma: 0.99
  alpha: 0.2
train_collector:
  _target_: tianshou.data.Collector
  _partial_: true
  buffer: 
    _target_: tianshou.data.VectorReplayBuffer
    total_size: 20000
    buffer_num: 2
  exploration_noise: true
test_collector:
  _target_: tianshou.data.Collector
  _partial_: true
trainer:
  _target_: tianshou.trainer.OffpolicyTrainer
  _partial_: true
  batch_size: 256
  step_per_epoch: 1000 # ~200/s
  max_epoch: 50
  episode_per_test: 2
  step_per_collect: 2000
  repeat_per_collect: 10



